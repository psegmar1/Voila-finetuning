{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12328927",
   "metadata": {},
   "source": [
    "# Fine-tuning of the Voila chat model for Covost-2 (Chinese to English)\n",
    "\n",
    "Para esta extensión de la tarea he elegido finetunear el modelo chat multimodal de [Voila](https://huggingface.co/maitrix-org/Voila-base). Antes de empezar, este modelo según su paper no es capaz de realizar **speech translation** pero con finetuning si que puede. Por este motivo, como es una nueva tarea si que emplearé todo el dataset de entrenamiento. Por último, este cuaderno se encuentra en el repositorio https://github.com/psegmar1/Voila-finetuning que es un fork del repo de Voila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f68a1c",
   "metadata": {},
   "source": [
    "## Importar todas las librerias y dependencias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f72785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:04:35.646010Z",
     "iopub.status.busy": "2025-12-24T13:04:35.645761Z",
     "iopub.status.idle": "2025-12-24T13:04:39.246016Z",
     "shell.execute_reply": "2025-12-24T13:04:39.244756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, default_data_collator\n",
    "from whisper_normalizer.basic import BasicTextNormalizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import librosa\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from voila_tokenizer import VoilaTokenizer\n",
    "from model import VoilaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb85fb",
   "metadata": {},
   "source": [
    "## Versión y dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc9b8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:04:39.249627Z",
     "iopub.status.busy": "2025-12-24T13:04:39.249367Z",
     "iopub.status.idle": "2025-12-24T13:04:39.356008Z",
     "shell.execute_reply": "2025-12-24T13:04:39.355068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.5.0+cu124\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version: \", torch. __version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef64838",
   "metadata": {},
   "source": [
    "## Cargar el dataset\n",
    "\n",
    "Con el fin de mantener la consistencia con los otros cuadernos el tamaño de dev será de 1000 muestras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404cd68c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:04:39.358363Z",
     "iopub.status.busy": "2025-12-24T13:04:39.358239Z",
     "iopub.status.idle": "2025-12-24T13:04:40.789345Z",
     "shell.execute_reply": "2025-12-24T13:04:40.788354Z"
    }
   },
   "outputs": [],
   "source": [
    "DEV_SIZE = 1_000\n",
    "seed = 42\n",
    "\n",
    "raw_datasets = load_dataset(\"facebook/covost2\", 'zh-CN_en', data_dir=\"/home/alumno.upv.es/psegmar1/TA_A3/cv-corpus-24.0-2025-12-05/zh-CN\", trust_remote_code=True)\n",
    "\n",
    "train_dataset = raw_datasets['train']\n",
    "dev_dataset = (\n",
    "    raw_datasets[\"validation\"]\n",
    "    .shuffle(seed=seed)\n",
    "    .select(range(DEV_SIZE))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c93181",
   "metadata": {},
   "source": [
    "## Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9babde5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:04:40.792060Z",
     "iopub.status.busy": "2025-12-24T13:04:40.791948Z",
     "iopub.status.idle": "2025-12-24T13:06:57.193168Z",
     "shell.execute_reply": "2025-12-24T13:06:57.192078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:  25%|██▌       | 1/4 [00:38<01:56, 38.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:  50%|█████     | 2/4 [01:18<01:18, 39.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards:  75%|███████▌  | 3/4 [01:57<00:39, 39.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards: 100%|██████████| 4/4 [02:10<00:00, 28.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading shards: 100%|██████████| 4/4 [02:10<00:00, 32.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"maitrix-org/Voila-chat\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = VoilaModel.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c011e",
   "metadata": {},
   "source": [
    "## Tokenizador del backbone LLM y de audio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6c38e",
   "metadata": {},
   "source": [
    "### Cargar ambos tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9ac9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:06:57.195912Z",
     "iopub.status.busy": "2025-12-24T13:06:57.195793Z",
     "iopub.status.idle": "2025-12-24T13:07:02.444032Z",
     "shell.execute_reply": "2025-12-24T13:07:02.443266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "VOILA_TOKENIZER_PATH = \"maitrix-org/Voila-Tokenizer\"\n",
    "\n",
    "tokenizer_llm_backbone = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "tokenizer_voila = VoilaTokenizer(model_path=VOILA_TOKENIZER_PATH, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786890f6",
   "metadata": {},
   "source": [
    "### Añadir nuevos tokens\n",
    "\n",
    "El LLM de Voila (de la familia de Llama) se entrena utilizando prefijos para las tareas que son tokens especiales. Por este motivo, he decidido añadir tres tokens nuevos al tokenizador del LLM que son el de la propia tarea, el que indica la lengua origen y el que cierra la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bdfdeb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:07:02.446449Z",
     "iopub.status.busy": "2025-12-24T13:07:02.446336Z",
     "iopub.status.idle": "2025-12-24T13:07:02.454073Z",
     "shell.execute_reply": "2025-12-24T13:07:02.453512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = ['<s2tt>', '<zh>', '<s2tt_text_output>']\n",
    "tokenizer_llm_backbone.add_special_tokens({'additional_special_tokens': new_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5951894",
   "metadata": {},
   "source": [
    "### Tokenizar el conjunto de train y validación para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd379d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:07:02.455918Z",
     "iopub.status.busy": "2025-12-24T13:07:02.455822Z",
     "iopub.status.idle": "2025-12-24T13:09:59.140636Z",
     "shell.execute_reply": "2025-12-24T13:09:59.139366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_for_training at 0x78d82419b100> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/7085 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  14%|█▍        | 1000/7085 [00:16<01:42, 59.22 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  28%|██▊       | 2000/7085 [00:30<01:15, 67.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  28%|██▊       | 2000/7085 [00:41<01:15, 67.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  42%|████▏     | 3000/7085 [00:42<00:55, 73.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  42%|████▏     | 3000/7085 [00:53<00:55, 73.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  56%|█████▋    | 4000/7085 [00:54<00:39, 77.55 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  71%|███████   | 5000/7085 [01:06<00:26, 79.44 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  85%|████████▍ | 6000/7085 [01:18<00:13, 79.08 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  99%|█████████▉| 7000/7085 [01:30<00:01, 82.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 7085/7085 [01:31<00:00, 81.71 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 7085/7085 [01:31<00:00, 77.54 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 77.51 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 77.36 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:   0%|          | 0/7085 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  14%|█▍        | 1000/7085 [00:05<00:30, 199.12 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  28%|██▊       | 2000/7085 [00:10<00:25, 196.71 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  42%|████▏     | 3000/7085 [00:14<00:19, 206.72 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  56%|█████▋    | 4000/7085 [00:19<00:14, 213.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  71%|███████   | 5000/7085 [00:24<00:09, 209.49 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  85%|████████▍ | 6000/7085 [00:29<00:05, 204.87 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:  99%|█████████▉| 7000/7085 [00:33<00:00, 212.14 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 7085/7085 [00:33<00:00, 213.46 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 7085/7085 [00:33<00:00, 209.21 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:05<00:00, 192.06 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:05<00:00, 191.98 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_START_TOKEN = \"<SYSTEM>\"\n",
    "DEFAULT_SYSTEM_END_TOKEN   = \"</SYSTEM>\"\n",
    "TASK_S2TT_TOKEN = \"<s2tt>\"\n",
    "TASK_S2TT_END_TOKEN = \"<s2tt_text_output>\"\n",
    "S2TT_CHINESE = \"<zh>\"\n",
    "DEFAULT_HUMAN_TOKEN = \"<|HUMAN|>\"\n",
    "DEFAULT_ASSISTANT_TOKEN = \"<|VOILA|>\"\n",
    "AUDIO_TOKEN_FORMAT = \"<|{}|>\"\n",
    "AUDIO_SR = 16000\n",
    "\n",
    "MAK_TOKEN_LEN = 512\n",
    "PAD_TOKEN_ID = tokenizer_llm_backbone.pad_token_id\n",
    "EOS_TOKEN_ID = tokenizer_llm_backbone.eos_token_id\n",
    "\n",
    "def _wrapper_audio_tokens(audio_tokens, num_codebooks, codebook_size):\n",
    "    ret_audio_tokens = []\n",
    "    for n in range(num_codebooks):\n",
    "        audio_token = audio_tokens[n]\n",
    "        ret_audio_tokens.append(''.join([AUDIO_TOKEN_FORMAT.format(au + n*codebook_size) if isinstance(au, int) else au for au in audio_token]))\n",
    "    return ret_audio_tokens\n",
    "\n",
    "def tokenize_for_training(samples):\n",
    "    num_codebooks = model.config.num_codebooks\n",
    "    codebook_size = model.config.codebook_size\n",
    "\n",
    "    system = DEFAULT_SYSTEM_START_TOKEN + TASK_S2TT_TOKEN + S2TT_CHINESE + TASK_S2TT_END_TOKEN + DEFAULT_SYSTEM_END_TOKEN\n",
    "\n",
    "    rv_input_ids = []\n",
    "    rv_label_ids = []\n",
    "    rv_attention_masks = []\n",
    "\n",
    "    total_samples = len(samples[\"file\"])\n",
    "\n",
    "    for i in range(total_samples):\n",
    "\n",
    "        # Copy into num_codebooks input ids\n",
    "        input_ids_list = []\n",
    "        for _ in range(num_codebooks):\n",
    "            input_ids_list.append([])\n",
    "\n",
    "        audio, _ = librosa.load(samples['file'][i], sr=AUDIO_SR)\n",
    "\n",
    "        # get audio token\n",
    "        audio_tokens = tokenizer_voila.encode(audio, sr=AUDIO_SR)\n",
    "        audio_tokens = audio_tokens.cpu().numpy().tolist()\n",
    "        audio_tokens = _wrapper_audio_tokens(audio_tokens, num_codebooks, codebook_size)\n",
    "        \n",
    "        labels = tokenizer_llm_backbone(samples['translation'][i], add_special_tokens=False)\n",
    "        base_label_ids = labels[\"input_ids\"] + [EOS_TOKEN_ID]\n",
    "\n",
    "        sample_attention_mask = []\n",
    "        sample_labels = []\n",
    "        set_attention_mask_and_labels = True\n",
    "        for n in range(num_codebooks):\n",
    "            content = system + DEFAULT_HUMAN_TOKEN + audio_tokens[n] + DEFAULT_ASSISTANT_TOKEN\n",
    "            content_ids = tokenizer_llm_backbone.encode(content, add_special_tokens=False, truncation=True,\n",
    "                                    max_length=tokenizer_llm_backbone.model_max_length)\n",
    "            \n",
    "            label_input_ids = base_label_ids + [EOS_TOKEN_ID]\n",
    "            model_inputs_ids = content_ids + label_input_ids\n",
    "            \n",
    "            if set_attention_mask_and_labels:\n",
    "                label_input_ids = [PAD_TOKEN_ID] * len(content_ids) + label_input_ids\n",
    "                sample_attention_mask = [1] * len(model_inputs_ids)\n",
    "                sample_attention_mask = [0] * (MAK_TOKEN_LEN - len(model_inputs_ids)) + sample_attention_mask\n",
    "                # Left padding\n",
    "                sample_labels =  [PAD_TOKEN_ID] * (\n",
    "                    MAK_TOKEN_LEN - len(label_input_ids)\n",
    "                ) + label_input_ids\n",
    "                set_attention_mask_and_labels = False\n",
    "\n",
    "            # Left padding\n",
    "            model_inputs_ids =  [PAD_TOKEN_ID] * (\n",
    "                MAK_TOKEN_LEN - len(model_inputs_ids)\n",
    "            ) + model_inputs_ids\n",
    "            \n",
    "            input_ids_list[n] += copy.deepcopy(model_inputs_ids)\n",
    "\n",
    "        for n in range(num_codebooks):\n",
    "            input_ids_list[n] = input_ids_list[n][:tokenizer_llm_backbone.model_max_length]\n",
    "\n",
    "        # To get [seq_len, num_codebooks]\n",
    "        transposed_inputs = list(map(list, zip(*input_ids_list)))\n",
    "        \n",
    "        rv_input_ids.append(transposed_inputs)\n",
    "        rv_label_ids.append(sample_labels)\n",
    "        rv_attention_masks.append(sample_attention_mask)\n",
    "\n",
    "    return {\"input_ids\": rv_input_ids, \"labels\": rv_label_ids, \"attention_mask\": rv_attention_masks}\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_for_training,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    tokenize_for_training,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# During tokenization I allow in inputs the max length of the model in order to discard now those that exceed my limit of MAK_TOKEN_LEN\n",
    "tokenized_train = tokenized_train.filter(lambda x: len(x[\"input_ids\"]) <= MAK_TOKEN_LEN and len(x[\"labels\"]) <= MAK_TOKEN_LEN , desc=f\"Discarding input with more than {MAK_TOKEN_LEN} tokens\")\n",
    "tokenized_dev = tokenized_dev.filter(lambda x: len(x[\"input_ids\"]) <= MAK_TOKEN_LEN and len(x[\"labels\"]) <= MAK_TOKEN_LEN , desc=f\"Discarding input with more than {MAK_TOKEN_LEN} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbb231",
   "metadata": {},
   "source": [
    "## Preparación para el finetuning\n",
    "\n",
    "En este caso al estar el modelo quantizado a 4 bits se está utilizando la técnica PEFT de QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec3e1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:09:59.143347Z",
     "iopub.status.busy": "2025-12-24T13:09:59.143214Z",
     "iopub.status.idle": "2025-12-24T13:09:59.475263Z",
     "shell.execute_reply": "2025-12-24T13:09:59.474115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,420,160 || all params: 8,162,145,280 || trainable%: 0.0419\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False, gradient_checkpointing_kwargs={'use_reentrant':False})\n",
    "\n",
    "# In this case this is not necessary because len(tokenizer_llm_backbone) < vocab_size. Moreover, I am not going to retrain the lm_head because the special\n",
    "# tokens I added are not going to be predicted\n",
    "\n",
    "# https://mohitmayank.com/a_lazy_data_science_guide/machine_learning/ML_snippets/#lora-with-selective-token-training\n",
    "# Resize model embedding matrix\n",
    "# model.resize_token_embeddings(len(tokenizer_llm_backbone))\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    trainable_token_indices={\n",
    "        'embed_tokens': tokenizer_llm_backbone.convert_tokens_to_ids(new_tokens)\n",
    "    },\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17eea7",
   "metadata": {},
   "source": [
    "### Más información sobre el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d7259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:09:59.478085Z",
     "iopub.status.busy": "2025-12-24T13:09:59.477973Z",
     "iopub.status.idle": "2025-12-24T13:09:59.486919Z",
     "shell.execute_reply": "2025-12-24T13:09:59.486144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_params': 4643124224, 'params_grad': 3420160, 'params_grad_name': ['base_model.model.model.embed_tokens.token_adapter.trainable_tokens_delta.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight']}\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): VoilaModel(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): TrainableTokensWrapper(\n",
      "          (original_module): None\n",
      "          (token_adapter): TrainableTokensLayer(\n",
      "            (base_layer): Embedding(136512, 4096)\n",
      "            (trainable_tokens_delta): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 3x4096 (cuda:0)])\n",
      "            (trainable_tokens_original): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 3x4096 (GPU 0)])\n",
      "          )\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=136512, bias=False)\n",
      "      (ref_emb_linear): Linear4bit(in_features=256, out_features=4096, bias=True)\n",
      "      (audio_transformer): AudioTransformer(\n",
      "        (embeddings): Embedding(2048, 1024)\n",
      "        (input_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerBlock(\n",
      "            (attention): Attention(\n",
      "              (wqkv): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
      "              (wo): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (feed_forward): FeedForward(\n",
      "              (w1): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
      "              (w3): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
      "              (w2): Linear4bit(in_features=2816, out_features=1024, bias=False)\n",
      "            )\n",
      "            (ffn_norm): RMSNorm()\n",
      "            (attention_norm): RMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "        (token_head): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def get_peft_model_sizes(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    params_grad = 0\n",
    "    params_grad_names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            params_grad += param.numel()\n",
    "            params_grad_names.append(name)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'params_grad': params_grad,\n",
    "        'params_grad_name': params_grad_names\n",
    "    }\n",
    "\n",
    "model_sizes = get_peft_model_sizes(lora_model)\n",
    "print(model_sizes)\n",
    "\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f339d78",
   "metadata": {},
   "source": [
    "### Definir función de pérdida\n",
    "\n",
    "Como en el propio codigo base no se calcula la loss para VoilaModel, defino una función para ello que recibirá el método encargado del entrenamiento. He preferido que esté aquí ya que quiero que este cuaderno sea bastante completo. Sin embargo, en el archivo 'model.py' si que he hecho algún cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41eff53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:09:59.489498Z",
     "iopub.status.busy": "2025-12-24T13:09:59.489394Z",
     "iopub.status.idle": "2025-12-24T13:09:59.492397Z",
     "shell.execute_reply": "2025-12-24T13:09:59.491713Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d49423",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e7e7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:09:59.494518Z",
     "iopub.status.busy": "2025-12-24T13:09:59.494421Z",
     "iopub.status.idle": "2025-12-24T17:34:02.475944Z",
     "shell.execute_reply": "2025-12-24T17:34:02.474576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2210' max='2210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2210/2210 4:23:48, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.920600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.920600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.525900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.525900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.379300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.379300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.296500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2210, training_loss=2.50663026835584, metrics={'train_runtime': 15842.459, 'train_samples_per_second': 4.472, 'train_steps_per_second': 0.139, 'total_flos': 6.589262311364493e+18, 'train_loss': 2.50663026835584, 'epoch': 9.957110609480813})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "model_name = CHECKPOINT.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-s2tt-zh-to-en\",\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=False, # If True => KeyError: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_loss_func=compute_loss_func\n",
    ")\n",
    "\n",
    "# Save the LLM tokenizer with the new tokens\n",
    "tokenizer_llm_backbone.save_pretrained(\"Voila-chat-tokenizer-s2tt\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0f030",
   "metadata": {},
   "source": [
    "## Cargar de nuevo el dataset y tokenizar el conjunto de dev y \n",
    "\n",
    "Con el fin de mantener la consistencia con los otros cuadernos el tamaño de dev y test será de 1000 muestras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db72940e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T17:34:02.480719Z",
     "iopub.status.busy": "2025-12-24T17:34:02.480576Z",
     "iopub.status.idle": "2025-12-24T17:35:00.084092Z",
     "shell.execute_reply": "2025-12-24T17:35:00.082534Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 82.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 82.80 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 83.22 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 83.15 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:03<00:00, 262.87 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:03<00:00, 262.72 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:03<00:00, 264.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Discarding input with more than 512 tokens: 100%|██████████| 1000/1000 [00:03<00:00, 264.51 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 1_000\n",
    "\n",
    "raw_datasets = load_dataset(\"facebook/covost2\", 'zh-CN_en', data_dir=\"/home/alumno.upv.es/psegmar1/TA_A3/cv-corpus-24.0-2025-12-05/zh-CN\", trust_remote_code=True)\n",
    "\n",
    "dev_dataset = (\n",
    "    raw_datasets[\"validation\"]\n",
    "    .shuffle(seed=seed)\n",
    "    .select(range(DEV_SIZE))\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    raw_datasets[\"test\"]\n",
    "    .shuffle(seed=seed)\n",
    "    .select(range(TEST_SIZE))\n",
    ")\n",
    "\n",
    "def tokenize_for_inference(samples):\n",
    "    num_codebooks = model.config.num_codebooks\n",
    "    codebook_size = model.config.codebook_size\n",
    "\n",
    "    system = DEFAULT_SYSTEM_START_TOKEN + TASK_S2TT_TOKEN + S2TT_CHINESE + TASK_S2TT_END_TOKEN + DEFAULT_SYSTEM_END_TOKEN\n",
    "\n",
    "    rv_input_ids = []\n",
    "\n",
    "    total_samples = len(samples[\"file\"])\n",
    "\n",
    "    for i in range(total_samples):\n",
    "\n",
    "        # Copy into num_codebooks input ids\n",
    "        input_ids_list = []\n",
    "        for _ in range(num_codebooks):\n",
    "            input_ids_list.append([])\n",
    "\n",
    "        audio, _ = librosa.load(samples['file'][i], sr=AUDIO_SR)\n",
    "\n",
    "        # get audio token\n",
    "        audio_tokens = tokenizer_voila.encode(audio, sr=AUDIO_SR)\n",
    "        audio_tokens = audio_tokens.cpu().numpy().tolist()\n",
    "        audio_tokens = _wrapper_audio_tokens(audio_tokens, num_codebooks, codebook_size)\n",
    "        \n",
    "        for n in range(num_codebooks):\n",
    "            content = system + DEFAULT_HUMAN_TOKEN + audio_tokens[n] + DEFAULT_ASSISTANT_TOKEN\n",
    "            content_ids = tokenizer_llm_backbone.encode(content, add_special_tokens=False, truncation=True,\n",
    "                                    max_length=tokenizer_llm_backbone.model_max_length)\n",
    "            \n",
    "            input_ids_list[n] += copy.deepcopy(content_ids)\n",
    "\n",
    "        for n in range(num_codebooks):\n",
    "            input_ids_list[n] = input_ids_list[n][:tokenizer_llm_backbone.model_max_length]\n",
    "        \n",
    "        rv_input_ids.append(input_ids_list)\n",
    "\n",
    "    return {\"input_ids\": rv_input_ids}\n",
    "\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    tokenize_for_inference,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    tokenize_for_inference,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# During tokenization I allow in inputs the max length of the model in order to discard now those that exceed my limit of MAK_TOKEN_LEN\n",
    "tokenized_test = tokenized_test.filter(lambda x: len(x[\"input_ids\"][0]) <= MAK_TOKEN_LEN, desc=f\"Discarding input with more than {MAK_TOKEN_LEN} tokens\")\n",
    "tokenized_dev = tokenized_dev.filter(lambda x: len(x[\"input_ids\"][0]) <= MAK_TOKEN_LEN, desc=f\"Discarding input with more than {MAK_TOKEN_LEN} tokens\")\n",
    "\n",
    "ds_dict = {\n",
    "    \"DEV\": tokenized_dev,\n",
    "    \"TEST\": tokenized_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8303949",
   "metadata": {},
   "source": [
    "## Cargar métricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2b7caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T17:35:00.087500Z",
     "iopub.status.busy": "2025-12-24T17:35:00.087358Z",
     "iopub.status.idle": "2025-12-24T17:35:08.246842Z",
     "shell.execute_reply": "2025-12-24T17:35:08.245637Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 65128.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = load(\"sacrebleu\")\n",
    "comet_metric = load(\"comet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553596f6",
   "metadata": {},
   "source": [
    "## Inferencia\n",
    "\n",
    "Debido a los métodos proporcionados por la clase de voila la inferencia es muestra a muestra y no por batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6151c86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T17:35:08.248378Z",
     "iopub.status.busy": "2025-12-24T17:35:08.248051Z",
     "iopub.status.idle": "2025-12-24T18:18:47.536481Z",
     "shell.execute_reply": "2025-12-24T18:18:47.535407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/alumno.upv.es/psegmar1/.conda/envs/voila_env/l ...\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU en DEV: 4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET en DEV: 57.45 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alumno.upv.es/psegmar1/.conda/envs/voila_env/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/alumno.upv.es/psegmar1/.conda/envs/voila_env/l ...\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU en TEST: 3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET en TEST: 55.70 %\n"
     ]
    }
   ],
   "source": [
    "num_codebooks = model.config.num_codebooks\n",
    "codebook_size = model.config.codebook_size\n",
    "\n",
    "DEFAULT_AUDIO_TOKEN = \"<au_token>\"\n",
    "\n",
    "AUDIO_MIN_TOKEN_ID = tokenizer_llm_backbone.convert_tokens_to_ids(AUDIO_TOKEN_FORMAT.format(0))\n",
    "AUDIO_MAX_TOKEN_ID = tokenizer_llm_backbone.convert_tokens_to_ids(AUDIO_TOKEN_FORMAT.format(codebook_size*num_codebooks-1))\n",
    "AUDIO_TOKEN_ID = tokenizer_llm_backbone.convert_tokens_to_ids(DEFAULT_AUDIO_TOKEN)\n",
    "ASSISTANT_TOKEN_ID = tokenizer_llm_backbone.convert_tokens_to_ids(DEFAULT_ASSISTANT_TOKEN)\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "for ds_name in [\"DEV\", \"TEST\"]:\n",
    "    data = ds_dict[ds_name]\n",
    "    total_samples = len(data[\"file\"])\n",
    "\n",
    "    hypothesis = []\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        input_ids = data[\"input_ids\"][i]\n",
    "        input_ids = torch.as_tensor([input_ids]).transpose(1,2).cuda()\n",
    "\n",
    "        gen_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"max_new_tokens\": MAK_TOKEN_LEN,\n",
    "            \"pad_token_id\": PAD_TOKEN_ID,\n",
    "            \"eos_token_id\": EOS_TOKEN_ID,\n",
    "            \"llm_audio_token_id\": AUDIO_TOKEN_ID,\n",
    "            \"min_audio_token_id\": AUDIO_MIN_TOKEN_ID,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_k\": 3,\n",
    "            \"use_audio_transformer\": False\n",
    "        }\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = lora_model.run_generate(**gen_params)\n",
    "\n",
    "            outputs = outputs[0].cpu().tolist()\n",
    "\n",
    "            predict_outputs = outputs[input_ids.shape[1]:]\n",
    "            text_outputs = []\n",
    "\n",
    "            for item in predict_outputs:\n",
    "                if item[0] != EOS_TOKEN_ID and not (item[0] >= AUDIO_MIN_TOKEN_ID and item[0] <= AUDIO_MAX_TOKEN_ID):\n",
    "                    text_outputs.append(item[0])\n",
    "\n",
    "            hypothesis.append(tokenizer_llm_backbone.decode(text_outputs))\n",
    "\n",
    "        \n",
    "    hypothesis_clean = [normalizer(text) for text in hypothesis]\n",
    "    sentence_clean = [normalizer(text) for text in data[\"sentence\"]]\n",
    "    translation_clean = [normalizer(text) for text in data[\"translation\"]]\n",
    "\n",
    "    result = bleu_metric.compute(predictions=hypothesis_clean, references=translation_clean)\n",
    "    print(f'BLEU en {ds_name}: {result[\"score\"]:.1f}')\n",
    "    comet_score = comet_metric.compute(predictions=hypothesis_clean, references=translation_clean, sources=sentence_clean)\n",
    "    print(f\"COMET en {ds_name}: {comet_score['mean_score'] * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
